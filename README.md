# Research-Paper-Note
My research paper notes, focusing on data mining/recommender/reinforcement learning. 我的论文笔记，主要聚焦于数据挖掘、推荐系统、强化学习

## An End-to-End Neighborhood-based Interaction Model for Knowledge-enhanced Recommendation

链接: [https://arxiv.org/abs/1908.04032](https://arxiv.org/abs/1908.04032)

关键词: NI, KNI, Graph, Recommender, KDD Workshop Best Paper

本文主要解决三个问题：

- Data Sparsity
- Cold Start
- Early Summarization

前两个是RecSys中常见的问题，一般GNN的方法都可以缓解，第三个问题则是本文提出的一个新概念，意思是寻常GNN的方法是直接利用目标结点的邻居去生成embedding，这样并没有充分的利用图结构的local structure（比如邻居与邻居之间的交互），因此这个embedding是被“过早”的生成了，缺失了一部分有价值的local信息。

接着，文章以Average Aggregation和Attention Aggregation为例，总结了目前GNN的一种通式：

![KNI-eq1](images/KNI-eq1.jpg)

![KNI-eq2](images/KNI-eq2.jpg)

其中矩阵A是一个系数矩阵，侧重于local structure；矩阵Z则是一对目标结点u，v的邻居的两两组合的内积矩阵，侧重于全局信息。而之前提到的Average Aggregation和Attention Aggregation，其实就是这个通式的特例，即改变矩阵A。因此我们也可以进一步对A进行改进，加入“邻居和邻居”之间的互动关系，从而得到NI模型的公式：

![KNI-eq1](images/KNI-eq3.jpg)

其实相对于Attention Aggregation，NI的不同就在于拼接了更多的结点信息。

上面的讨论还是仅限于User-Item的图结构，如果我们通过引入知识图谱的信息，即更多的Entity和Relation，就可以构建KNI。相比于NI，KNI的区别就是图结构更加丰富，公式和计算过程没有改变。除此之外，KNI和NI都运用了Neighbor Sampling的技术。